{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example starting state\n",
    "env.s = env.encode(3, 1, 2, 0) # 328\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "        \n",
    "def print_agent_run_starting_at_state(env, agent, state, sleep_val = 0.01):\n",
    "    env.s = state  # set environment to illustration's state\n",
    "    epochs = 0\n",
    "    penalties, reward = 0, 0\n",
    "    frames = [] # for animation\n",
    "    done = False\n",
    "    while not done:\n",
    "        state, reward, done, info = agent.act(env, state)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(\"Timestep:\" + str(i + 1))\n",
    "        print(\"State: \" + str(frame['state']))\n",
    "        print(\"Reward: \" + str(frame['reward']))\n",
    "        sleep(sleep_val)\n",
    "\n",
    "    print(\"Timesteps taken: {}\".format(epochs))\n",
    "    print(\"Penalties incurred: {}\".format(penalties))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "Timestep:619\n",
      "State: 324\n",
      "Reward: -1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-009cd2a3c9a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Animation showing behavior of RandomAgent. Warning: May take several thousand timesteps to complete!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint_agent_run_starting_at_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m328\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-9dad57c47cb9>\u001b[0m in \u001b[0;36mprint_agent_run_starting_at_state\u001b[0;34m(env, agent, state, sleep_val)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"State: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reward: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timesteps taken: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define random agent that chooses a random action at every step\n",
    "class RandomAgent:\n",
    "    def act(self, env, state):\n",
    "        action = env.action_space.sample()\n",
    "        return env.step(action)\n",
    "    \n",
    "# Animation showing behavior of RandomAgent. Warning: May take several thousand timesteps to complete!\n",
    "print_agent_run_starting_at_state(env, RandomAgent(), 328)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Basic Q Learning algorithm, as described here:\n",
    "# https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "\n",
    "def q_learning_train(env, alpha = 0.1, gamma = 0.6, epsilon = 0.1, epochs = 30000):\n",
    "    # Q Table is initialized to all zeroes\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    # Training using Q-Learning\n",
    "    for i in range(1, epochs + 1):\n",
    "        state = env.reset()\n",
    "\n",
    "        epochs, penalties, reward, = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample() # Explore action space\n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "            next_state, reward, done, info = env.step(action) \n",
    "\n",
    "            old_value = q_table[state, action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "\n",
    "            # Q learning equation\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state, action] = new_value\n",
    "\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "\n",
    "            state = next_state\n",
    "            epochs += 1\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Episode: \" + str(i))\n",
    "\n",
    "    print(\"Training finished.\\n\")\n",
    "    return q_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, episodes = 100):\n",
    "    total_epochs, total_penalties, total_reward = 0, 0, 0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        epochs, penalties, reward = 0, 0, 0\n",
    "\n",
    "        done = False\n",
    "        reward = 0\n",
    "\n",
    "        while not done:\n",
    "            state, reward, done, info = agent.act(env, state)\n",
    "\n",
    "            if reward == -10 or (done and reward == 0): # TODO: don't hard-code penalty calculation but put it on env object\n",
    "                penalties += 1\n",
    "\n",
    "            epochs += 1\n",
    "            if epochs > 10000:\n",
    "                penalties += 10000\n",
    "                break\n",
    "\n",
    "        total_penalties += penalties\n",
    "        total_epochs += epochs\n",
    "        total_reward += reward\n",
    "\n",
    "    print(\"Results after \" + str(episodes) + \" episodes\")\n",
    "    print(\"Average reward per episode: \" + str(total_reward / episodes))\n",
    "    print(\"Average timesteps per episode: \" + str(total_epochs / episodes))\n",
    "    print(\"Average penalties per episode: \" + str(total_penalties / episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 30000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 16.1 s, sys: 0 ns, total: 16.1 s\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "taxi_q_table = q_learning_train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep:10\n",
      "State: 0\n",
      "Reward: 20\n",
      "Timesteps taken: 10\n",
      "Penalties incurred: 0\n"
     ]
    }
   ],
   "source": [
    "# Define a new agent that uses trained Q Table for all decisions\n",
    "class QAgent:\n",
    "    def __init__(self, q_table):\n",
    "        self.q_table = q_table\n",
    "    \n",
    "    def act(self, env, state):\n",
    "        action = np.argmax(self.q_table[state])\n",
    "        return env.step(action)\n",
    "\n",
    "# Show animation with behavior of this agent\n",
    "print_agent_run_starting_at_state(env, QAgent(taxi_q_table), 328, sleep_val=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for RandomAgent\n",
      "Results after 200 episodes\n",
      "Average reward per episode: 20.0\n",
      "Average timesteps per episode: 2144.225\n",
      "Average penalties per episode: 694.63\n",
      "\n",
      "Evaluation for QAgent\n",
      "Results after 200 episodes\n",
      "Average reward per episode: 20.0\n",
      "Average timesteps per episode: 12.185\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation for RandomAgent\")\n",
    "evaluate_agent(env, RandomAgent(), 200)\n",
    "\n",
    "print(\"\\nEvaluation for QAgent\")\n",
    "evaluate_agent(env, QAgent(taxi_q_table), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep:12\n",
      "State: 410\n",
      "Reward: 20\n",
      "Timesteps taken: 12\n",
      "Penalties incurred: 0\n"
     ]
    }
   ],
   "source": [
    "# Show 10 random episodes\n",
    "for _ in range(10):\n",
    "    print_agent_run_starting_at_state(env, QAgent(taxi_q_table), env.reset(), sleep_val=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake8x8-v0\").env\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFF\u001b[41mH\u001b[0mFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "Timestep:31\n",
      "State: 19\n",
      "Reward: 0.0\n",
      "Timesteps taken: 31\n",
      "Penalties incurred: 0\n"
     ]
    }
   ],
   "source": [
    "print_agent_run_starting_at_state(env, RandomAgent(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adopting Q Learning for the different environment, that only has 1 reward at very end\n",
    "def q_learning_train2(env, alpha = 0.1, gamma = 0.6, epsilon = 0.1, epochs = 30000):\n",
    "    # Q Table is initialized to all zeroes\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    # Training using Q-Learning\n",
    "    for i in range(1, epochs + 1):\n",
    "        state = env.reset()\n",
    "\n",
    "        epochs, reward, = 0, 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample() # Explore action space\n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "            next_state, reward, done, info = env.step(action) \n",
    "            \n",
    "            # This is the only difference from previous algorithm; we need a way to code negative reward for holes\n",
    "            # If we had control of the env, we wouldn't need this hack.\n",
    "            if done and reward == 0:\n",
    "                reward = -1\n",
    "\n",
    "            old_value = q_table[state, action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "\n",
    "            # Q learning equation\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state, action] = new_value\n",
    "\n",
    "            state = next_state\n",
    "            epochs += 1\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Episode: \" + str(i))\n",
    "\n",
    "    print(\"Training finished.\\n\")\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 5000\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_table2 = q_learning_train2(env, gamma=0.9, epochs = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFFF\u001b[41mF\u001b[0mFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "Timestep:262\n",
      "State: 5\n",
      "Reward: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-43f3a9521dfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Show animation with behavior of this agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint_agent_run_starting_at_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msleep_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-9dad57c47cb9>\u001b[0m in \u001b[0;36mprint_agent_run_starting_at_state\u001b[0;34m(env, agent, state, sleep_val)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"State: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reward: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Timesteps taken: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Show animation with behavior of this agent\n",
    "print_agent_run_starting_at_state(env, QAgent(q_table2), 0, sleep_val=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for RandomAgent\n",
      "Results after 200 episodes\n",
      "Average reward per episode: 0.0\n",
      "Average timesteps per episode: 30.68\n",
      "Average penalties per episode: 1.0\n",
      "\n",
      "Evaluation for QAgent\n",
      "Results after 200 episodes\n",
      "Average reward per episode: 0.875\n",
      "Average timesteps per episode: 103.4\n",
      "Average penalties per episode: 0.125\n"
     ]
    }
   ],
   "source": [
    "# Here the \"average reward\" is going to be a percentage of how many successful\n",
    "# and \"average penalties\" will be how many failed.\n",
    "print(\"Evaluation for RandomAgent\")\n",
    "evaluate_agent(env, RandomAgent(), 200)\n",
    "\n",
    "print(\"\\nEvaluation for QAgent\")\n",
    "evaluate_agent(env, QAgent(q_table2), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20000\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frozen_q_table_5k = q_learning_train2(env, gamma=0.6, epochs = 5000)\n",
    "frozen_q_table_10k = q_learning_train2(env, gamma=0.6, epochs = 10000)\n",
    "frozen_q_table_20k = q_learning_train2(env, gamma=0.6, epochs = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation for QAgent (5k epoch)\n",
      "Results after 200 episodes\n",
      "Average reward per episode: 0.915\n",
      "Average timesteps per episode: 88.51\n",
      "Average penalties per episode: 0.085\n",
      "\n",
      "Evaluation for QAgent (10k epoch)\n",
      "Results after 200 episodes\n",
      "Average reward per episode: 0.835\n",
      "Average timesteps per episode: 125.93\n",
      "Average penalties per episode: 0.165\n",
      "\n",
      "Evaluation for QAgent (20k epoch)\n",
      "Results after 200 episodes\n",
      "Average reward per episode: 0.8\n",
      "Average timesteps per episode: 79.635\n",
      "Average penalties per episode: 0.2\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluation for QAgent (5k epoch)\")\n",
    "evaluate_agent(env, QAgent(frozen_q_table_5k), 200)\n",
    "\n",
    "print(\"\\nEvaluation for QAgent (10k epoch)\")\n",
    "evaluate_agent(env, QAgent(frozen_q_table_10k), 200)\n",
    "\n",
    "print(\"\\nEvaluation for QAgent (20k epoch)\")\n",
    "evaluate_agent(env, QAgent(frozen_q_table_20k), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation for QAgent (5k epoch)\n",
      "Results after 200 episodes\n",
      "Average reward per episode: 0.61\n",
      "Average timesteps per episode: 107.645\n",
      "Average penalties per episode: 0.39\n",
      "\n",
      "Evaluation for QAgent (10k epoch)\n",
      "Results after 200 episodes\n",
      "Average reward per episode: 0.84\n",
      "Average timesteps per episode: 119.985\n",
      "Average penalties per episode: 0.16\n",
      "\n",
      "Evaluation for QAgent (20k epoch)\n",
      "Results after 200 episodes\n",
      "Average reward per episode: 0.655\n",
      "Average timesteps per episode: 90.66\n",
      "Average penalties per episode: 0.345\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluation for QAgent (5k epoch)\")\n",
    "evaluate_agent(env, QAgent(frozen_q_table_5k), 200)\n",
    "\n",
    "print(\"\\nEvaluation for QAgent (10k epoch)\")\n",
    "evaluate_agent(env, QAgent(frozen_q_table_10k), 200)\n",
    "\n",
    "print(\"\\nEvaluation for QAgent (20k epoch)\")\n",
    "evaluate_agent(env, QAgent(frozen_q_table_20k), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
